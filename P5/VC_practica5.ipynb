{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fe3a9f",
   "metadata": {},
   "source": [
    "### Entrenamiento Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf7f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dasum\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Iniciando extracci√≥n de caracter√≠sticas con VGG-Face...\n",
      "\n",
      "Procesando categor√≠a: neutral (603 im√°genes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/603 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-11-13 16:08:37 - üîó vgg_face_weights.h5 will be downloaded from https://github.com/serengil/deepface_models/releases/download/v1.0/vgg_face_weights.h5 to C:\\Users\\dasum\\.deepface\\weights\\vgg_face_weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/vgg_face_weights.h5\n",
      "To: C:\\Users\\dasum\\.deepface\\weights\\vgg_face_weights.h5\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 580M/580M [00:20<00:00, 27.8MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 603/603 [04:01<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando categor√≠a: sonriendo (600 im√°genes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 600/600 [03:24<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de caracter√≠sticas extra√≠das: (1203, 4096)\n",
      "Total de etiquetas: (1203,)\n",
      "\n",
      "Evaluando modelo con split 80/20...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.97      0.97      0.97       121\n",
      "   sonriendo       0.97      0.97      0.97       120\n",
      "\n",
      "    accuracy                           0.97       241\n",
      "   macro avg       0.97      0.97      0.97       241\n",
      "weighted avg       0.97      0.97      0.97       241\n",
      "\n",
      "\n",
      "Entrenando modelo final con TODOS los datos...\n",
      "\n",
      "¬°√âxito! Modelo guardado en 'smile_classifier.pkl'\n",
      "Mapeo de categor√≠as guardado en 'categories.pkl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from deepface import DeepFace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "DATA_DIR = \"data\"\n",
    "CATEGORIES = [\"neutral\", \"sonriendo\"] # 0 = neutral, 1 = sonriendo\n",
    "MODEL_NAME = \"VGG-Face\"\n",
    "MODEL_FILENAME = \"smile_classifier.pkl\"\n",
    "CATEGORIES_FILENAME = \"categories.pkl\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(f\"Iniciando extracci√≥n de caracter√≠sticas con {MODEL_NAME}...\")\n",
    "\n",
    "# Recorrer categor√≠as y procesar im√°genes\n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    path = os.path.join(DATA_DIR, category)\n",
    "    image_files = os.listdir(path)\n",
    "    print(f\"\\nProcesando categor√≠a: {category} ({len(image_files)} im√°genes)\")\n",
    "    \n",
    "    for img_name in tqdm(image_files):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        \n",
    "        try:\n",
    "            # 'represent' detecta, alinea y extrae el embedding\n",
    "            embedding_obj = DeepFace.represent(img_path=img_path, \n",
    "                                               model_name=MODEL_NAME, \n",
    "                                               enforce_detection=False)\n",
    "            \n",
    "            # embedding_obj es una lista, tomamos el primero\n",
    "            embedding_vector = embedding_obj[0]['embedding']\n",
    "            \n",
    "            X.append(embedding_vector)\n",
    "            y.append(i) # 0 para neutral, 1 para sonriendo\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {img_path}: {e}\")\n",
    "\n",
    "if not X:\n",
    "    print(\"Error: No se extrajo ninguna caracter√≠stica. ¬øEst√°n las im√°genes en la carpeta 'data'?\")\n",
    "    exit()\n",
    "\n",
    "# Convertir a arrays de numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"\\nTotal de caracter√≠sticas extra√≠das: {X.shape}\")\n",
    "print(f\"Total de etiquetas: {y.shape}\")\n",
    "\n",
    "# --- Evaluaci√≥n (Opcional pero recomendado) ---\n",
    "print(\"\\nEvaluando modelo con split 80/20...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "eval_model = SVC(kernel='linear', probability=True)\n",
    "eval_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = eval_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=CATEGORIES))\n",
    "\n",
    "\n",
    "# --- Entrenamiento del Modelo Final ---\n",
    "print(\"\\nEntrenando modelo final con TODOS los datos...\")\n",
    "final_model = SVC(kernel='linear', probability=True)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Guardar el modelo\n",
    "joblib.dump(final_model, MODEL_FILENAME)\n",
    "# Guardar las categor√≠as (para saber qu√© significa 0 y 1)\n",
    "joblib.dump(CATEGORIES, CATEGORIES_FILENAME)\n",
    "\n",
    "print(f\"\\n¬°√âxito! Modelo guardado en '{MODEL_FILENAME}'\")\n",
    "print(f\"Mapeo de categor√≠as guardado en '{CATEGORIES_FILENAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cef5f",
   "metadata": {},
   "source": [
    "### Demo en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90428874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado. Iniciando demo en vivo...\n",
      "Pulsa 'q' para salir.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "MODEL_FILENAME = \"smile_classifier.pkl\"\n",
    "CATEGORIES_FILENAME = \"categories.pkl\"\n",
    "EMOJI_FILE = \"happy.jpeg\"\n",
    "MODEL_NAME = \"VGG-Face\" # Debe ser el mismo usado en el entrenamiento\n",
    "DETECTOR_BACKEND = \"opencv\" # 'mtcnn' o 'opencv'\n",
    "CONF_THRESHOLD = 0.70 # Umbral de confianza para mostrar la reacci√≥n\n",
    "\n",
    "# --- Helper para superponer emoji ---\n",
    "def overlay_transparent(background_img, overlay_img, x, y):\n",
    "    \"\"\" Superpone una imagen (con canal alfa) sobre otra. \"\"\"\n",
    "    try:\n",
    "        # Asegurar que el overlay no se salga de los l√≠mites\n",
    "        bg_h, bg_w, _ = background_img.shape\n",
    "        ol_h, ol_w, ol_c = overlay_img.shape\n",
    "\n",
    "        if x < 0: x = 0\n",
    "        if y < 0: y = 0\n",
    "        if x + ol_w > bg_w: ol_w = bg_w - x\n",
    "        if y + ol_h > bg_h: ol_h = bg_h - y\n",
    "\n",
    "        # Recortar el overlay si es necesario\n",
    "        overlay_img = overlay_img[0:ol_h, 0:ol_w]\n",
    "\n",
    "        if ol_c == 4: # Si tiene canal alfa\n",
    "            alpha_s = overlay_img[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "\n",
    "            roi = background_img[y:y+ol_h, x:x+ol_w]\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                roi[:, :, c] = (alpha_s * overlay_img[:, :, c] +\n",
    "                                alpha_l * roi[:, :, c])\n",
    "\n",
    "            background_img[y:y+ol_h, x:x+ol_w] = roi\n",
    "    except Exception as e:\n",
    "        print(f\"Error al superponer imagen: {e}\")\n",
    "    \n",
    "    return background_img\n",
    "\n",
    "# --- Carga de recursos ---\n",
    "try:\n",
    "    model = joblib.load(MODEL_FILENAME)\n",
    "    CATEGORIES = joblib.load(CATEGORIES_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontraron los archivos '{MODEL_FILENAME}' o '{CATEGORIES_FILENAME}'.\")\n",
    "    print(\"Por favor, ejecuta el script '02_entrenar_modelo.py' primero.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    emoji = cv2.imread(EMOJI_FILE, -1) # -1 para cargar canal alfa\n",
    "    if emoji is None: raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    print(f\"Advertencia: No se encontr√≥ '{EMOJI_FILE}'. La reacci√≥n ser√° solo texto.\")\n",
    "    emoji = None\n",
    "\n",
    "print(\"Modelo cargado. Iniciando demo en vivo...\")\n",
    "print(\"Pulsa 'q' para salir.\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # 'represent' encuentra la cara y extrae el embedding en un solo paso\n",
    "        embedding_objs = DeepFace.represent(frame,\n",
    "                                          model_name=MODEL_NAME,\n",
    "                                          detector_backend=DETECTOR_BACKEND,\n",
    "                                          enforce_detection=False)\n",
    "        \n",
    "        # 'represent' devuelve una lista, tomamos el primer (y usualmente √∫nico) resultado\n",
    "        if len(embedding_objs) > 0:\n",
    "            obj = embedding_objs[0]\n",
    "            embedding_vector = obj['embedding']\n",
    "            \n",
    "            facial_area = obj['facial_area']\n",
    "            x = facial_area['x']\n",
    "            y = facial_area['y']\n",
    "            w = facial_area['w']\n",
    "            h = facial_area['h']\n",
    "\n",
    "            # Predecir con el modelo\n",
    "            prediction = model.predict([embedding_vector])\n",
    "            proba = model.predict_proba([embedding_vector])\n",
    "\n",
    "            label_index = prediction[0]\n",
    "            label_name = CATEGORIES[label_index]\n",
    "            confidence = proba[0][label_index]\n",
    "            \n",
    "            # --- LA REACCI√ìN ---\n",
    "            if label_name == \"sonriendo\" and confidence > CONF_THRESHOLD:\n",
    "                text = f\"SONRIENDO ({confidence*100:.0f}%)\"\n",
    "                color = (0, 255, 0)\n",
    "                \n",
    "                # Reacci√≥n 1: Emoji\n",
    "                if emoji is not None:\n",
    "                    emoji_size = w // 2 # Tama√±o del emoji relativo a la cara\n",
    "                    resized_emoji = cv2.resize(emoji, (emoji_size, emoji_size))\n",
    "                    # Posici√≥n (esquina superior derecha de la cara)\n",
    "                    emoji_x = x + w - (emoji_size // 2)\n",
    "                    emoji_y = y - (emoji_size // 2)\n",
    "                    frame = overlay_transparent(frame, resized_emoji, emoji_x, emoji_y)\n",
    "            else:\n",
    "                text = f\"{label_name.upper()} ({confidence*100:.0f}%)\"\n",
    "                color = (0, 0, 255)\n",
    "            \n",
    "            # Dibujar siempre el bounding box y el texto\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # A veces el detector falla\n",
    "        print(f\"Error en el bucle principal: {e}\")\n",
    "        pass\n",
    "\n",
    "    cv2.imshow(\"Prototipo 1 - Detector de Sonrisas\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
