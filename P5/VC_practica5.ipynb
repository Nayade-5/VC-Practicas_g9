{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fe3a9f",
   "metadata": {},
   "source": [
    "### Entrenamiento Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extracciÃ³n de caracterÃ­sticas con VGG-Face...\n",
      "\n",
      "Procesando categorÃ­a: neutral (603 imÃ¡genes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 603/603 [02:43<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando categorÃ­a: sonriendo (600 imÃ¡genes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [04:54<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando categorÃ­a: triste (599 imÃ¡genes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 599/599 [02:31<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de caracterÃ­sticas extraÃ­das: (1802, 4096)\n",
      "Total de etiquetas: (1802,)\n",
      "\n",
      "Evaluando modelo con split 80/20...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.89      0.97      0.93       121\n",
      "   sonriendo       0.97      0.95      0.96       120\n",
      "      triste       0.97      0.91      0.94       120\n",
      "\n",
      "    accuracy                           0.94       361\n",
      "   macro avg       0.94      0.94      0.94       361\n",
      "weighted avg       0.94      0.94      0.94       361\n",
      "\n",
      "\n",
      "Entrenando modelo final con TODOS los datos...\n",
      "\n",
      "Â¡Ã‰xito! Modelo guardado en 'smile_classifier.pkl'\n",
      "Mapeo de categorÃ­as guardado en 'categories.pkl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from deepface import DeepFace\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config \n",
    "DATA_DIR = \"data\"\n",
    "CATEGORIES = [\"neutral\", \"sonriendo\", \"triste\"] # 0 = neutral, 1 = sonriendo, 2 = triste\n",
    "MODEL_NAME = \"VGG-Face\"\n",
    "MODEL_FILENAME = \"smile_classifier.pkl\"\n",
    "CATEGORIES_FILENAME = \"categories.pkl\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(f\"Iniciando entrenamiento\")\n",
    "\n",
    "# Recorrer imÃ¡genes \n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    path = os.path.join(DATA_DIR, category)\n",
    "    image_files = os.listdir(path)\n",
    "    print(f\"\\nProcesando categorÃ­a: {category} ({len(image_files)} imÃ¡genes)\")\n",
    "    \n",
    "    for img_name in tqdm(image_files):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        \n",
    "        try:\n",
    "            embedding_obj = DeepFace.represent(img_path=img_path, \n",
    "                                               model_name=MODEL_NAME, \n",
    "                                               enforce_detection=False)\n",
    "            \n",
    "            # embedding_obj es una lista, se toma el primero\n",
    "            embedding_vector = embedding_obj[0]['embedding']\n",
    "            \n",
    "            X.append(embedding_vector)\n",
    "            y.append(i) # 0 para neutral, 1 para sonriendo, 2 para triste\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {img_path}: {e}\")\n",
    "\n",
    "if not X:\n",
    "    print(\"No se extrajo ninguna caracterÃ­stica\")\n",
    "    exit()\n",
    "\n",
    "# Convertir a arrays de numpy\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"\\nTotal de caracterÃ­sticas extraÃ­das: {X.shape}\")\n",
    "print(f\"Total de etiquetas: {y.shape}\")\n",
    "\n",
    "# EvaluaciÃ³n\n",
    "print(\"\\nEvaluando modelo con split 80/20...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "eval_model = SVC(kernel='linear', probability=True)\n",
    "eval_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = eval_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=CATEGORIES))\n",
    "\n",
    "\n",
    "print(\"\\nEntrenando modelo final\")\n",
    "final_model = SVC(kernel='linear', probability=True)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "joblib.dump(final_model, MODEL_FILENAME)\n",
    "joblib.dump(CATEGORIES, CATEGORIES_FILENAME)\n",
    "\n",
    "print(f\"\\n Modelo guardado en '{MODEL_FILENAME}'\")\n",
    "print(f\"CategorÃ­as disponibles: '{CATEGORIES_FILENAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cef5f",
   "metadata": {},
   "source": [
    "### Demo en vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90428874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado. Iniciando demo en vivo...\n",
      "Pulsa 'q' para salir.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Config \n",
    "MODEL_FILENAME = \"smile_classifier.pkl\"\n",
    "CATEGORIES_FILENAME = \"categories.pkl\"\n",
    "SAD_EMOJI_FILE = \"pepo.png\"\n",
    "EMOJI_FILE = \"happy_face.png\"\n",
    "MODEL_NAME = \"VGG-Face\" \n",
    "DETECTOR_BACKEND = \"opencv\" \n",
    "CONF_THRESHOLD = 0.70 \n",
    "\n",
    "# Rendimiento\n",
    "FRAME_SKIP_RATE = 5  # Analizar 1 de cada 5 fotogramas\n",
    "\n",
    "# FunciÃ³n para superponer imÃ¡genes\n",
    "def overlay_transparent(background_img, overlay_img, x, y):\n",
    "    \"\"\" Superpone una imagen (con canal alfa) sobre otra. \"\"\"\n",
    "    try:\n",
    "        bg_h, bg_w, _ = background_img.shape\n",
    "        ol_h, ol_w, ol_c = overlay_img.shape\n",
    "\n",
    "        if x < 0: x = 0\n",
    "        if y < 0: y = 0\n",
    "        if x + ol_w > bg_w: ol_w = bg_w - x\n",
    "        if y + ol_h > bg_h: ol_h = bg_h - y\n",
    "\n",
    "        overlay_img = overlay_img[0:ol_h, 0:ol_w]\n",
    "\n",
    "        if ol_c == 4: # Si tiene canal alfa\n",
    "            alpha_s = overlay_img[:, :, 3] / 255.0\n",
    "            alpha_l = 1.0 - alpha_s\n",
    "            roi = background_img[y:y+ol_h, x:x+ol_w]\n",
    "            for c in range(0, 3):\n",
    "                roi[:, :, c] = (alpha_s * overlay_img[:, :, c] +\n",
    "                                alpha_l * roi[:, :, c])\n",
    "            background_img[y:y+ol_h, x:x+ol_w] = roi\n",
    "    except Exception as e:\n",
    "        print(f\"Error al superponer imagen: {e}\")\n",
    "    \n",
    "    return background_img\n",
    "\n",
    "# Carga de recursos \n",
    "try:\n",
    "    model = joblib.load(MODEL_FILENAME)\n",
    "    CATEGORIES = joblib.load(CATEGORIES_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontraron los archivos '{MODEL_FILENAME}' o '{CATEGORIES_FILENAME}'.\")\n",
    "    exit()\n",
    "\n",
    "# Cargar emojis\n",
    "try:\n",
    "    emoji = cv2.imread(EMOJI_FILE, -1)\n",
    "    if emoji is None: raise FileNotFoundError(EMOJI_FILE)\n",
    "    sad_emoji = cv2.imread(SAD_EMOJI_FILE, -1)\n",
    "    if sad_emoji is None: raise FileNotFoundError(SAD_EMOJI_FILE)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"No se encontrÃ³ el emoji\")\n",
    "    if str(e) == EMOJI_FILE:\n",
    "         emoji = None\n",
    "    if str(e) == SAD_EMOJI_FILE:\n",
    "         sad_emoji = None\n",
    "\n",
    "print(\"Pulsa 'q' para salir.\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Variables  \n",
    "frame_count = 0\n",
    "last_box = None \n",
    "last_label_name = \"neutral\"\n",
    "last_confidence = 0.0\n",
    "last_color = (0, 0, 255)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # BÃºsqueda / anÃ¡lisis\n",
    "    if frame_count % FRAME_SKIP_RATE == 0:\n",
    "        try:\n",
    "            embedding_objs = DeepFace.represent(frame,\n",
    "                                                  model_name=MODEL_NAME,\n",
    "                                                  detector_backend=DETECTOR_BACKEND,\n",
    "                                                  enforce_detection=False)\n",
    "            \n",
    "            if len(embedding_objs) > 0:\n",
    "                obj = embedding_objs[0]\n",
    "                facial_area = obj['facial_area']\n",
    "                \n",
    "                # Guardar cara\n",
    "                last_box = (facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h'])\n",
    "                \n",
    "                embedding_vector = obj['embedding']\n",
    "                prediction = model.predict([embedding_vector])\n",
    "                proba = model.predict_proba([embedding_vector])\n",
    "                label_index = prediction[0]\n",
    "                \n",
    "                last_label_name = CATEGORIES[label_index]\n",
    "                last_confidence = proba[0][label_index]\n",
    "                \n",
    "                if last_label_name == \"sonriendo\" and last_confidence > CONF_THRESHOLD:\n",
    "                    last_color = (0, 255, 0)\n",
    "                elif last_label_name == \"triste\" and last_confidence > CONF_THRESHOLD:\n",
    "                    last_color = (255, 0, 0)\n",
    "                else:\n",
    "                    last_color = (0, 0, 255)\n",
    "            \n",
    "            else:\n",
    "                last_box = None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en el bucle de anÃ¡lisis: {e}\")\n",
    "            last_box = None\n",
    "\n",
    "    \n",
    "    # Renderizado\n",
    "    if last_box is not None:\n",
    "        x, y, w, h = last_box\n",
    "        \n",
    "        text = f\"{last_label_name.upper()} ({last_confidence*100:.0f}%)\"\n",
    "        \n",
    "        # ReacciÃ³n \n",
    "        if last_label_name == \"sonriendo\" and last_confidence > CONF_THRESHOLD:\n",
    "            if emoji is not None:\n",
    "                emoji_size = w // 2\n",
    "                resized_emoji = cv2.resize(emoji, (emoji_size, emoji_size))\n",
    "                emoji_x = x + w - (emoji_size // 2)\n",
    "                emoji_y = y - (emoji_size // 2)\n",
    "                frame = overlay_transparent(frame, resized_emoji, emoji_x, emoji_y)\n",
    "\n",
    "        elif last_label_name == \"triste\" and last_confidence > CONF_THRESHOLD:\n",
    "            if sad_emoji is not None:\n",
    "                emoji_size = w // 2\n",
    "                resized_emoji = cv2.resize(sad_emoji, (emoji_size, emoji_size))\n",
    "                emoji_x = x + w - (emoji_size // 2)\n",
    "                emoji_y = y - (emoji_size // 2)\n",
    "                frame = overlay_transparent(frame, resized_emoji, emoji_x, emoji_y)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), last_color, 2)\n",
    "        cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, last_color, 2)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Detector de Sonrisas - 'q' para salir\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcdba5",
   "metadata": {},
   "source": [
    "### Prototipo Libre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e77dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜œ Face Controller Deluxe iniciado â€” pulsa 'q' para salir.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.82it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.38it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.04it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.33it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.15it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.15it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.12it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.79it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.88it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.02it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.91it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.55it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.25it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.45it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.45it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.82it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.86it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.78it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.81it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.08it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.81it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.01it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.53it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.88it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.79it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.85it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.79it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.81it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.47it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.80it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.51it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.99it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.89it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.89it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.13it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.01it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.97it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.69it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.12it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.29it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.70it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.54it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.58it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.11it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.20it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.60it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.10it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.12it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.25it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.55it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.66it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.99it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.21it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.70it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.09it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.33it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.57it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.20it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.82it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.43it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.52it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.98it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.95it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.72it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.65it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.13it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.07it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.20it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.78it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.63it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.80it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.69it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.02it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.28it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.53it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.94it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.85it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.09it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.03it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.06it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.85it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.12it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.61it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.44it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.71it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.23it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.67it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.63it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.56it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.78it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.25it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.72it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.41it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.13it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.91it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.84it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.59it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.86it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.07it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.64it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.67it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.72it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.90it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.04it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.19it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.05it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  6.08it/s]\n",
      "Action: gender: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  5.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from mtcnn import MTCNN\n",
    "import random\n",
    "\n",
    "# ImÃ¡genes\n",
    "GLASSES_FILE = \"swagger_glasses.png\"\n",
    "CHAR_FILE = \"pepo.png\"\n",
    "CHAR_MALE_FILE = \"bigote.png\"   \n",
    "CHAR_FEMALE_FILE = \"lazo.png\"\n",
    "CHAR_SPY_FILE = \"pepo_spy.png\"\n",
    "\n",
    "# Detecciones cada 15 fotogramas (motivos de rendimiento)\n",
    "FRAME_SKIP_RATE = 15  \n",
    "\n",
    "# ParÃ¡metros para ajustes de los efectos\n",
    "CONF_THRESHOLD = 50\n",
    "GLASSES_SPEED = 15\n",
    "HEAD_TURN_THRESHOLD = 0.18\n",
    "\n",
    "# FunciÃ³n de superposiciÃ³n de imÃ¡genes \n",
    "def overlay_transparent(bg, overlay, x, y, scale=1.0):\n",
    "    try:\n",
    "        if overlay is None:\n",
    "            return bg\n",
    "        \n",
    "        h_overlay, w_overlay, c_overlay = overlay.shape\n",
    "        new_w = int(w_overlay * scale)\n",
    "        new_h = int(h_overlay * scale)\n",
    "\n",
    "        if new_w <= 0 or new_h <= 0:\n",
    "            return bg\n",
    "            \n",
    "        overlay = cv2.resize(overlay, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        h, w = new_h, new_w\n",
    "\n",
    "        if c_overlay < 4:\n",
    "            return bg\n",
    "\n",
    "        rows, cols, _ = bg.shape\n",
    "        y1, y2 = max(0, y), min(rows, y + h)\n",
    "        x1, x2 = max(0, x), min(cols, x + w)\n",
    "\n",
    "        y1o, y2o = max(0, -y), h - max(0, y + h - rows)\n",
    "        x1o, x2o = max(0, -x), w - max(0, x + w - cols)\n",
    "\n",
    "        if y1o < y2o and x1o < x2o:\n",
    "            alpha = overlay[y1o:y2o, x1o:x2o, 3] / 255.0\n",
    "            alpha_inv = 1.0 - alpha\n",
    "\n",
    "            roi = bg[y1:y2, x1:x2]\n",
    "            overlay_rgb = overlay[y1o:y2o, x1o:x2o, :3]\n",
    "\n",
    "            for c in range(3):\n",
    "                roi[:, :, c] = (alpha * overlay_rgb[:, :, c] +\n",
    "                                alpha_inv * roi[:, :, c])\n",
    "            \n",
    "            bg[y1:y2, x1:x2] = roi\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al superponer imagen: {e}\")\n",
    "    return bg\n",
    "\n",
    "# Cargar ImÃ¡genes  \n",
    "glasses = cv2.imread(GLASSES_FILE, -1)\n",
    "character = cv2.imread(CHAR_FILE, -1)\n",
    "char_male = cv2.imread(CHAR_MALE_FILE, -1)\n",
    "char_female = cv2.imread(CHAR_FEMALE_FILE, -1)\n",
    "char_spy = cv2.imread(CHAR_SPY_FILE, -1)\n",
    "\n",
    "# DepuraciÃ³n \n",
    "if glasses is None: \n",
    "    print(\"No se encontrÃ³ la imagen de gafas.\")\n",
    "if character is None: \n",
    "    print(\"No se encontrÃ³ la imagen de personaje.\")\n",
    "if char_male is None or char_female is None:\n",
    "    print(\"No se encontrÃ³ el gÃ©nero. Saliendo.\")\n",
    "    exit()\n",
    "if char_spy is None:\n",
    "    print(\"No se encontrÃ³ pepo_spy.png.\")\n",
    "\n",
    "# CÃ¡mara en vivo \n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"No se encontrÃ³ una fuente de vÃ­deo.\")\n",
    "    exit()\n",
    "\n",
    "print(\" Meme Face Control Iniciado  â€” pulsa 'q' para salir.\")\n",
    "\n",
    "# InicializaciÃ³n  \n",
    "detector = MTCNN()\n",
    "\n",
    "frame_count = 0\n",
    "last_box = None\n",
    "last_landmarks = None\n",
    "last_dominant = \"neutral\"\n",
    "last_gender = \"Man\"\n",
    "glasses_pos = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    # DetecciÃ³n de cara y landmarks en cada frame\n",
    "    try:\n",
    "        faces = detector.detect_faces(frame)\n",
    "        if faces:\n",
    "            face = faces[0]\n",
    "            last_box = face['box']\n",
    "            last_landmarks = face['keypoints']\n",
    "        else:\n",
    "            last_box = None\n",
    "            last_landmarks = None\n",
    "    except Exception as e:\n",
    "        last_box = None\n",
    "        last_landmarks = None\n",
    "        print(f\"Error MTCNN: {e}\")\n",
    "    \n",
    "    if frame_count % FRAME_SKIP_RATE == 0 and last_box is not None:\n",
    "        try:\n",
    "            x, y, w, h = last_box\n",
    "            if w > 0 and h > 0:\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "\n",
    "                result = DeepFace.analyze(face_img,\n",
    "                                          actions=['emotion', 'gender'],\n",
    "                                          enforce_detection=False,\n",
    "                                          detector_backend='skip')\n",
    "                \n",
    "                emotions = result[0][\"emotion\"]\n",
    "                dominant = max(emotions, key=emotions.get)\n",
    "                if emotions[dominant] < CONF_THRESHOLD:\n",
    "                    last_dominant = \"neutral\"\n",
    "                else:\n",
    "                    last_dominant = dominant\n",
    "                \n",
    "                last_gender = result[0].get('dominant_gender',\n",
    "                                            result[0].get('gender', 'Man'))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error de anÃ¡lisis (DeepFace): {e}\")\n",
    "\n",
    "    # Render\n",
    "    if last_box is not None:\n",
    "        x, y, w, h = last_box\n",
    "        \n",
    "\n",
    "        if last_dominant == 'happy' and glasses is not None and last_landmarks:\n",
    "            g_w = w\n",
    "            g_h = int(glasses.shape[0] * (g_w / glasses.shape[1]))\n",
    "            gx = x\n",
    "            \n",
    "            le_y = last_landmarks['left_eye'][1]\n",
    "            re_y = last_landmarks['right_eye'][1]\n",
    "            offset_vertical = - (h // 5) \n",
    "            target_y = (le_y + re_y) // 2 + offset_vertical\n",
    "\n",
    "            if glasses_pos is None:\n",
    "                glasses_pos = y - g_h\n",
    "\n",
    "            if glasses_pos < target_y:\n",
    "                glasses_pos += GLASSES_SPEED\n",
    "                if glasses_pos > target_y:\n",
    "                    glasses_pos = target_y\n",
    "\n",
    "            frame = overlay_transparent(frame, glasses, gx, glasses_pos,\n",
    "                                        scale=g_w / glasses.shape[1])\n",
    "        else:\n",
    "            glasses_pos = None\n",
    "\n",
    "        # Si last_gender es Woman aÃ±ade bigote, sino un lazo\n",
    "        if last_dominant == 'surprise' and last_landmarks:\n",
    "            if last_gender == 'Woman':\n",
    "                nose_x, nose_y = last_landmarks['nose']\n",
    "                scale = (w * 0.5) / char_male.shape[1]\n",
    "                sw = int(char_male.shape[1] * scale)\n",
    "                frame = overlay_transparent(frame, char_male,\n",
    "                                            nose_x - sw // 2,\n",
    "                                            nose_y,\n",
    "                                            scale=scale)\n",
    "            else:\n",
    "                le_x, le_y = last_landmarks['left_eye']\n",
    "                scale = (w * 0.4) / char_female.shape[1]\n",
    "                sw = int(char_female.shape[1] * scale)\n",
    "                sh = int(char_female.shape[0] * scale)\n",
    "                frame = overlay_transparent(frame, char_female,\n",
    "                                            le_x - sw // 2,\n",
    "                                            le_y - sh - (h // 20),\n",
    "                                            scale=scale)\n",
    "\n",
    "\n",
    "        # Pepo espÃ­a\n",
    "        if last_landmarks and char_spy is not None:\n",
    "            le_x = last_landmarks['left_eye'][0]\n",
    "            re_x = last_landmarks['right_eye'][0]\n",
    "            nose_x = last_landmarks['nose'][0]\n",
    "\n",
    "            eye_dist = re_x - le_x\n",
    "\n",
    "            if eye_dist > 10:\n",
    "                eye_center_x = (le_x + re_x) / 2\n",
    "                turn_offset = nose_x - eye_center_x\n",
    "                normalized_turn = turn_offset / eye_dist\n",
    "\n",
    "                spy_scale = 0.25\n",
    "                spy_h = frame.shape[0]\n",
    "                spy_y = int(spy_h * 0.40)\n",
    "\n",
    "                if normalized_turn > HEAD_TURN_THRESHOLD:\n",
    "                    frame = overlay_transparent(frame, char_spy,\n",
    "                                                0, spy_y, scale=spy_scale)\n",
    "\n",
    "                elif normalized_turn < -HEAD_TURN_THRESHOLD:\n",
    "                    flipped_spy = cv2.flip(char_spy, 1)\n",
    "                    spy_w = int(flipped_spy.shape[1] * spy_scale)\n",
    "                    frame = overlay_transparent(frame, flipped_spy,\n",
    "                                                frame.shape[1] - spy_w,\n",
    "                                                spy_y,\n",
    "                                                scale=spy_scale)\n",
    "\n",
    "    cv2.imshow(\"Face Meme Control - 'q' para salir\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
